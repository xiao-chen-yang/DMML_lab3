# k-NN classification: Iris dataset

`Iris` flower dataset consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters. 

Let's start with an exploratory analysis on this data before building a classification model using $k$-NN.

## Exploratory data analysis \label{sec:EDA}

First, we can calculate some summary statistics for this data. 

```{r}
library(skimr)
skim(iris)
```

Next, we use `ggpairs` to create plots for continuous variables by groups. 

```{r}
library(GGally)
ggpairs(iris, columns=1:3, ggplot2::aes(colour=Species, alpha=0.2))
```

## Classification using k-NN

### Data splitting

We now divide the Iris data into training, validation and test sets to apply $k$-NN classification. 50% of the data is used for training, 25% is used for selecting the optimal $k$, and the remaining 25% of the data is used to evaluate the performance of $k$-NN. 

```{r}
set.seed(1)
n <- nrow(iris)
ind1 <- sample(c(1:n), floor(0.5*n))
ind2 <- sample(c(1:n)[-ind1], floor(0.25*n))
ind3 <- setdiff(c(1:n),c(ind1,ind2))
iris.train <- iris[ind1,]
iris.valid <- iris[ind2,]
iris.test  <- iris[ind3,]
```

### Distances

One important decision to be made when applying $k$-NN is the distance measure. By default, `knn` uses the Euclidean distance. 

Looking at the summary statistics computed earlier, we see that Iris features have different ranges and standard deviations. This raises a concern of directly using the Euclidean distance, since features with large ranges and/or standard deviations may become a dominant term in the calculation of Euclidean distance. A simple remedy for this is to standardise^[Suppose we have a set of observations $X=\{x_1, x_2, \ldots, x_n\}$. To standardise the variable, we subtract its mean value and divide by its standard deviation. That is, $$x'_i = \frac{x_i-\text{mean}{X}}{\text{sd}(X)}$$] all features so that they all have mean zero and variance of one.
```{r}
var.mean <- apply(iris.train[,1:4],2,mean) #calculate mean of each feature
var.sd   <- apply(iris.train[,1:4],2,sd)   #calculate standard deviation of each feature

# standardise training, validation and test sets
iris.train.scale <-t(apply(iris.train[,1:4], 1, function(x) (x-var.mean)/var.sd))
iris.valid.scale <-t(apply(iris.valid[,1:4], 1, function(x) (x-var.mean)/var.sd))
iris.test.scale  <-t(apply(iris.test[,1:4],  1, function(x) (x-var.mean)/var.sd))
```

We will not discuss using other distances for $k$-NN in this course. If you are interested, please check the following links for codes and examples.

1. [kknn: Weighted k-Nearest Neighbor Classifier](https://www.rdocumentation.org/packages/kknn/versions/1.3.1/topics/kknn)
The `kknn` packages allows computing (weighted) Minkowski distance, which includes the Euclidean distance and Manhattan distances as special cases. 

2. [How to code kNN algorithm in R from scratch](https://anderfernandez.com/en/blog/code-knn-in-r/)
This tutorial explains the mechanism of $k$NN from scatch and shows ways to perform $k$-NN for any defined distances. 

### Finding the optimal value of k
Now we evaluate the error rate on the validation set for different values of k and plot the error rate against $k$. 
```{r}
library(class)
K <- c(1:15)
valid.error <- c()
for (k in K){
  valid.pred <- knn(iris.train.scale, iris.valid.scale, iris.train[,5], k=k)
  valid.error[k] <- mean(iris.valid[,5] != valid.pred)
}

plot(K, valid.error, type="b", ylab="validation error rate")
```

**QUESTION**: Which value of $k$ would you select for $k$-NN? $k=$ `r fitb(c(8,9))`

`r hide("Solution")`
$k=8$ or $k=9$ gives the optimal performance on the validation set. For now, we just pick $k=8$ to be our final classifier.  
`r unhide()`

### Prediction

Finally we can apply $8$-NN to the test set and see how accurate our classifier is. 

```{r}
k.opt <- which.min(valid.error)
test.pred <- knn(iris.train.scale, iris.test.scale, iris.train[,5], k=k.opt)
table(iris.test$Species,test.pred)
```

Our classifier achieves 100% accuracy, which is perfect!

## Task

Considering the dataset is relatively small, we may use cross-validation to help decide $k$. In this case, we only need to split the data into training and test sets.  
```{r}
set.seed(1)
n <- nrow(iris)
ind <- sample(c(1:n), floor(0.8*n))
iris.train <- iris[ind,]
iris.test  <- iris[-ind,]
```

(a) Write a piece of code to standardise features in the training data and test data. 

```{r webex.hide="Solution"}
var.mean <- apply(iris.train[,1:4],2,mean) #calculate mean of each feature
var.sd   <- apply(iris.train[,1:4],2,sd)   #calculate standard deviation of each feature

# standardise training, validation and test sets
iris.train.scale <-t(apply(iris.train[,1:4], 1, function(x) (x-var.mean)/var.sd))
iris.test.scale  <-t(apply(iris.test[,1:4],  1, function(x) (x-var.mean)/var.sd))
```


(b) Use leave-one-out cross-validation to decide the optimal value of $k$. 

`r hide("Hint")`
Use `knn.cv` for leave-one-out cross-validation
`r unhide()`

```{r webex.hide="Solution"}
K <- c(1:15)
cv.error <- c()
for (k in K){
  train.pred <- knn.cv(iris.train.scale, iris.train[,5], k=k)
  cv.error[k] <- mean(iris.train[,5] != train.pred)
}

plot(K, cv.error, type="b", ylab="validation error rate")
k.opt <- which.min(cv.error); print(k.opt)
```

