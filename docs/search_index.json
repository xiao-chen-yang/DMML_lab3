[["index.html", "STATS5099 Data Mining and Machine Learning 1 Welcome to DMML Lab 3 1.1 k-NN 1.2 Linear and quadratic discriminant analysis", " STATS5099 Data Mining and Machine Learning 1 Welcome to DMML Lab 3 In week 3, we have studied \\(k\\)-nearest neighbours (\\(k\\)-NN) and linear/quadratic discriminant analysis (LDA/QDA). We also introduced measures to evaluate the performance of classifiers, such as (mis)correct classification rate, class-specific rates, sensitivity and specificity, ROC and AUC, and two data splitting approaches, namely training, validation and test split, and cross-validation. Before reviewing specific classifiers, let's first summarise the general steps for building and evaluating classifiers. Suppose we have divided the data into training, validation and test sets. Then, the procedure is as follows. Build the classifier on the training data. Use the classifier built in step 1 to make predictions for data in the validation set and evaluate its performance. (Implement steps 1 and 2 for all proposed classifiers and different parameter values in the classifier, e.g. \\(k\\) in \\(k\\)-NN.) Select the optimal classifier and its parameters (if any) to be the one with the highest correct classification rate (or some other appropriate evaluation metrics) on the validation set. Re-run the selected classifier on the test set to make predictions. In the case that the data set is small, \\(K\\)-fold cross-validation may be used instead of training and validation split. This essentially means repeating steps 1 and 2 for \\(K\\) rounds, where in each round, use \\((K-1)/K\\) proportion of data for building the classifier and the remaining \\(1/K\\) proportion of data for evaluating the classifier. The final validation performance is the mean correct classification rate averaged over \\(K\\) rounds. Once the optimal classifier is selected (i.e. step 3), we can proceed to make predictions (i.e. step 4). The R command for manually splitting the data into training, validation and test sets is given below. n &lt;- nrow(data) #sample size ind1 &lt;- sample(c(1:n), floor(train.prop * n)) #train.prop stands for the proportion of data in the training set ind2 &lt;- sample(c(1:n)[-ind1], floor(valid.prop * n)) #valid.prop stands for the proportion of data in the validation set ind3 &lt;- setdiff(c(1:n),c(ind1,ind2)) train.data &lt;- data[ind1, ] valid.data &lt;- data[ind2, ] test.data &lt;- data[ind3, ] # Remark: The floor() function is used to round any number to integers. There are also built-in functions for data splitting in many R packages. For example, in the SDMTune package. # The following codes are non-examinable. library(SDMtune) datasets &lt;- trainValTest(data, val = valid.prop, test = test.prop) train.data &lt;- datasets[[1]] val.data &lt;- datasets[[2]] test.data &lt;- datasets[[3]] 1.1 k-NN Suppose we have training features train.X, training labels train.Y, validation features valid.X and validation labels valid.Y. The \\(k\\)-NN classifier can be built by using library(class) valid.pred &lt;- knn(train.X, valid.X, train.Y, k=k) #k is the number of neighbours considered R also have built-in functions for performing 1-NN and leave-one-out cross-validation of \\(k\\)-NN. # 1-NN valid.pred.1NN &lt;- knn1(train.X, valid.X, train.Y) # leave-one-out cross-validation of k-NN cv &lt;- knn.cv(train.X, train.Y, k=k) To decide the value of \\(k\\) in \\(k\\)-NN, we will follow steps 1 and 2 as described above. That is, evaluate the performance of \\(k\\)-NN on the validation set or using cross-validation for a range of \\(k\\) and select the optimal \\(k\\) that returns the highest validation correct classification rate. 1.2 Linear and quadratic discriminant analysis Before implementing LDA and QDA, it is important to check if the assumptions are satisfied, i.e. the feature vectors follow multivariate Gaussian distributions, and additionally for LDA, the covariance matrices are equal across classes. In R, two useful commands for checking these assumptions are: # calculate variance by group aggregate(x, by=list(factor), FUN=var) #&#39;x&#39; is a feature vector and &#39;factor&#39; is used to group features; for classification, the class label can be used as &#39;factor&#39;. # density plots, scatterplot, and Pearson correlation library(GGally) ggpairs(data, ggplot2::aes(colour=factor)) #create plots by groups # You could also add the argument &#39;columns&#39; to specify which columns to be plotted, e.g. ggpairs(data, columns=2:3, ggplot2::aes(colour=factor)) #plotting 2nd and 3rd columns The syntax for applying LDA and QDA is same as building a regression model: LDA &lt;- lda(Y ~ X1 + X2 + X3, data) QDA &lt;- qda(Y ~ X1 + X2 + X3, data) By default, the prior probabilities of each class are estimated as the class proportions. It can be specified explicitly by adding the argument prior. For example, assume we have a binary classification problem with equal prior for each class. Then we may change the code to: LDA &lt;- lda(Y ~ X1 + X2 + X3, data, prior=c(1/2,1/2)) "],["k-nn-classification-iris-dataset.html", "2 k-NN classification: Iris dataset 2.1 Exploratory data analysis 2.2 Classification using k-NN 2.3 Task", " 2 k-NN classification: Iris dataset Iris flower dataset consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters. Let's start with an exploratory analysis on this data before building a classification model using \\(k\\)-NN. 2.1 Exploratory data analysis First, we can calculate some summary statistics for this data. library(skimr) skim(iris) Table 2.1: Data summary Name iris Number of rows 150 Number of columns 5 _______________________ Column type frequency: factor 1 numeric 4 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts Species 0 1 FALSE 3 set: 50, ver: 50, vir: 50 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist Sepal.Length 0 1 5.84 0.83 4.3 5.1 5.80 6.4 7.9 ▆▇▇▅▂ Sepal.Width 0 1 3.06 0.44 2.0 2.8 3.00 3.3 4.4 ▁▆▇▂▁ Petal.Length 0 1 3.76 1.77 1.0 1.6 4.35 5.1 6.9 ▇▁▆▇▂ Petal.Width 0 1 1.20 0.76 0.1 0.3 1.30 1.8 2.5 ▇▁▇▅▃ Next, we use ggpairs to create plots for continuous variables by groups. library(GGally) ## Loading required package: ggplot2 ## Registered S3 method overwritten by &#39;GGally&#39;: ## method from ## +.gg ggplot2 ggpairs(iris, columns=1:3, ggplot2::aes(colour=Species, alpha=0.2)) 2.2 Classification using k-NN 2.2.1 Data splitting We now divide the Iris data into training, validation and test sets to apply \\(k\\)-NN classification. 50% of the data is used for training, 25% is used for selecting the optimal \\(k\\), and the remaining 25% of the data is used to evaluate the performance of \\(k\\)-NN. set.seed(1) n &lt;- nrow(iris) ind1 &lt;- sample(c(1:n), floor(0.5*n)) ind2 &lt;- sample(c(1:n)[-ind1], floor(0.25*n)) ind3 &lt;- setdiff(c(1:n),c(ind1,ind2)) iris.train &lt;- iris[ind1,] iris.valid &lt;- iris[ind2,] iris.test &lt;- iris[ind3,] 2.2.2 Distances One important decision to be made when applying \\(k\\)-NN is the distance measure. By default, knn uses the Euclidean distance. Looking at the summary statistics computed earlier, we see that Iris features have different ranges and standard deviations. This raises a concern of directly using the Euclidean distance, since features with large ranges and/or standard deviations may become a dominant term in the calculation of Euclidean distance. A simple remedy for this is to standardise1 all features so that they all have mean zero and variance of one. var.mean &lt;- apply(iris.train[,1:4],2,mean) #calculate mean of each feature var.sd &lt;- apply(iris.train[,1:4],2,sd) #calculate standard deviation of each feature # standardise training, validation and test sets iris.train.scale &lt;-t(apply(iris.train[,1:4], 1, function(x) (x-var.mean)/var.sd)) iris.valid.scale &lt;-t(apply(iris.valid[,1:4], 1, function(x) (x-var.mean)/var.sd)) iris.test.scale &lt;-t(apply(iris.test[,1:4], 1, function(x) (x-var.mean)/var.sd)) We will not discuss using other distances for \\(k\\)-NN in this course. If you are interested, please check the following links for codes and examples. kknn: Weighted k-Nearest Neighbor Classifier The kknn packages allows computing (weighted) Minkowski distance, which includes the Euclidean distance and Manhattan distances as special cases. How to code kNN algorithm in R from scratch This tutorial explains the mechanism of \\(k\\)NN from scatch and shows ways to perform \\(k\\)-NN for any defined distances. 2.2.3 Finding the optimal value of k Now we evaluate the correct classification rate on the validation set for different values of k and plot the correct classification rate against \\(k\\). library(class) set.seed(1) K &lt;- c(1:15) valid.corr &lt;- c() for (k in K){ valid.pred &lt;- knn(iris.train.scale, iris.valid.scale, iris.train[,5], k=k) valid.corr[k] &lt;- mean(iris.valid[,5] == valid.pred) } plot(K, valid.corr, type=&quot;b&quot;, ylab=&quot;validation correct classification rate&quot;) QUESTION: Which value of \\(k\\) would you select for \\(k\\)-NN? \\(k=\\) Solution \\(k=9\\) gives the optimal performance on the validation set. Note that if we re-run the code with different initialisation (i.e. by changing the value in set.seed), the optimal value of \\(k\\) might change. 2.2.4 Prediction Finally we can apply \\(3\\)-NN to the test set and see how accurate our classifier is. k.opt &lt;- which.max(valid.corr) test.pred &lt;- knn(iris.train.scale, iris.test.scale, iris.train[,5], k=k.opt) table(iris.test$Species,test.pred) ## test.pred ## setosa versicolor virginica ## setosa 13 0 0 ## versicolor 0 14 0 ## virginica 0 0 11 Our classifier achieves 100% accuracy, which is perfect! 2.3 Task Considering the dataset is relatively small, we may use cross-validation to help decide \\(k\\). In this case, we only need to split the data into training and test sets. set.seed(1) n &lt;- nrow(iris) ind &lt;- sample(c(1:n), floor(0.8*n)) iris.train &lt;- iris[ind,] iris.test &lt;- iris[-ind,] Write a piece of code to standardise features in the training data and test data. Solution var.mean &lt;- apply(iris.train[,1:4],2,mean) #calculate mean of each feature var.sd &lt;- apply(iris.train[,1:4],2,sd) #calculate standard deviation of each feature # standardise training, validation and test sets iris.train.scale &lt;-t(apply(iris.train[,1:4], 1, function(x) (x-var.mean)/var.sd)) iris.test.scale &lt;-t(apply(iris.test[,1:4], 1, function(x) (x-var.mean)/var.sd)) Use leave-one-out cross-validation to decide the optimal value of \\(k\\). Hint Use knn.cv for leave-one-out cross-validation Solution K &lt;- c(1:15) cv.corr &lt;- c() for (k in K){ train.pred &lt;- knn.cv(iris.train.scale, iris.train[,5], k=k) cv.corr[k] &lt;- mean(iris.train[,5] == train.pred) } plot(K, cv.corr, type=&quot;b&quot;, ylab=&quot;leave-one-out cross-validation correct classification rate&quot;) k.opt &lt;- which.max(cv.corr); print(k.opt) ## [1] 6 Suppose we have a set of observations \\(X=\\{x_1, x_2, \\ldots, x_n\\}\\). To standardise the variable, we subtract its mean value and divide by its standard deviation. That is, \\[x&#39;_i = \\frac{x_i-\\text{mean}(X)}{\\text{sd}(X)}\\]↩︎ "],["lda-and-qda-iris-dataset.html", "3 LDA and QDA: Iris dataset 3.1 Checking assumptions 3.2 LDA 3.3 Task", " 3 LDA and QDA: Iris dataset Let's continue the analysis on Iris dataset, but now using discriminant analysis. 3.1 Checking assumptions The ggpairs plot produced in previous section suggests that the Gaussian assumption is roughly satisfied. In addition, we also need to check if the groups have equal covariance. for (i in 1:4) { cat(colnames(iris)[i],sep=&quot;\\n&quot;) print(aggregate(iris[,i],by=list(iris$Species),var)) #compute variance for each group } ## Sepal.Length ## Group.1 x ## 1 setosa 0.1242 ## 2 versicolor 0.2664 ## 3 virginica 0.4043 ## Sepal.Width ## Group.1 x ## 1 setosa 0.14369 ## 2 versicolor 0.09847 ## 3 virginica 0.10400 ## Petal.Length ## Group.1 x ## 1 setosa 0.03016 ## 2 versicolor 0.22082 ## 3 virginica 0.30459 ## Petal.Width ## Group.1 x ## 1 setosa 0.01111 ## 2 versicolor 0.03911 ## 3 virginica 0.07543 Results suggest that considerable differences in the variances for Sepal Length and Petal Length. Therefore, it may not be very appropriate to apply LDA. Nonetheless, to illustrate how LDA and QDA works, we will run both models in this example. 3.2 LDA Since there is no parameter to select in LDA or QDA, we only need to split the data into training and test sets. set.seed(1) n &lt;- nrow(iris) ind &lt;- sample(c(1:n), floor(0.8*n)) iris.train &lt;- iris[ind,] iris.test &lt;- iris[-ind,] Next, we implement LDA by using the lda function. library(MASS) iris.lda &lt;- lda(Species~., data=iris.train) iris.lda ## Call: ## lda(Species ~ ., data = iris.train) ## ## Prior probabilities of groups: ## setosa versicolor virginica ## 0.3250 0.3167 0.3583 ## ## Group means: ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## setosa 5.008 3.421 1.462 0.2513 ## versicolor 5.976 2.768 4.308 1.3211 ## virginica 6.647 2.984 5.595 2.0442 ## ## Coefficients of linear discriminants: ## LD1 LD2 ## Sepal.Length 0.7682 -0.1347 ## Sepal.Width 1.6720 -2.0470 ## Petal.Length -2.2383 1.0337 ## Petal.Width -2.7061 -2.9118 ## ## Proportion of trace: ## LD1 LD2 ## 0.9906 0.0094 Since we have three classes, two (\\(3-1\\)) linear discriminant functions are produced. We could see how the first linear discriminant function separates the three classes by using ldahist function or by using ggplot. iris.pred.tr &lt;- predict(iris.lda) ldahist(data = iris.pred.tr$x[,1], g=iris.train$Species) dataset &lt;- data.frame(Type=iris.train$Species, lda=iris.pred.tr$x) ggplot(dataset, aes(x=lda.LD1)) + geom_density(aes(group=Type, colour=Type, fill=Type), alpha=0.3) The first linear discriminant is quite effective in separating setosa from the other classes, but less effective in separating versicolor and virginica. While R could automatically help us make predictions, let's try to manually create a classification rule using only LD1 based on the figure above. # The classification rule is manually designed as follows: # classify observations to virginica if LD1 &lt; -3.5; # classify observations to versicolor if -3.5 &lt;= LD1 &lt;= 3; # classify observations to setosa if LD1 &gt; 3 pred.LD1 &lt;-rep(&quot;versicolor&quot;,nrow(iris.train)) pred.LD1[iris.pred.tr$x[,1] &lt; -3.5] &lt;- &quot;virginica&quot; pred.LD1[iris.pred.tr$x[,1] &gt; 3] &lt;- &quot;setosa&quot; mean(pred.LD1==iris.train$Species) ## [1] 0.9833 This shows that we have a correct classification rate of 98.3% on the training data. Challenge: How might you decide the optimal cutting points instead of using \\(-3.5\\) and \\(3\\) in the classification rule above? Let's now move to using two linear discriminant functions and see if that improves the classification accuracy. First, we could visualise how LD1 and LD2 together separate the three classes by using either of the following two comments # plot(iris.pred.tr$x[,1],iris.pred.tr$x[,2], col=train.data$Species, # pch=as.numeric(train.data$Species), xlab=&quot;LD1&quot;, ylab=&quot;LD2&quot;) ggplot(dataset, aes(x=lda.LD1, y=lda.LD2)) + geom_point(aes(group=Type, colour=Type, shape=Type)) Now let's compute the correct classification rate again, but using both LD1 and LD2. mean(iris.train$Species == iris.pred.tr$class) ## [1] 0.975 The correct classification rate even decreases slightly compared with using LD1. Finally, let's use the built LDA to predict species for test data. The correct classification rate is 100%, great! iris.pred.te &lt;- predict(iris.lda, iris.test) mean(iris.test$Species == iris.pred.te$class) ## [1] 1 3.3 Task Implement QDA and compute the correct classification rate on the test set. Solution iris.qda &lt;- qda(Species~., data=iris.train) iris.pred.te2 &lt;- predict(iris.qda, iris.test) mean(iris.test$Species == iris.pred.te2$class) ## [1] 0.9667 "],["exercise-haberman.html", "4 Exercise: Haberman", " 4 Exercise: Haberman The Haberman dataset contains cases from a study that was conducted between 1958 and 1970 at the University of Chicago's Billings Hospital on the survival of patients who had undergone surgery for breast cancer. The dataset contains information about four variables: V1. Age of patient at time of operation (numerical) V2. Patient's year of operation (year - 1900, numerical) V3. Number of positive axillary nodes detected (numerical) V4. Survival status (class label) 1 = the patient survived 5 years or longer 2 = the patient died within 5 year To load the data into R and convert V4 (i.e. class) from numeric variable to categorical variable, use haberman &lt;- read.table(&quot;haberman.data&quot;,sep = &quot;,&quot;) haberman$V4 &lt;- as.factor(haberman$V4) QUESTION Perform exploratory analysis for this data. What have you observed? Hint For numerical summaries, the following may be attempted summary(haberman[,1:3]) skim(haberman) apply(haberman[,1:3], 2, function(x) aggregate(x,by=list(haberman$V4), var)) For graphical summaries, try apply(haberman[,1:3],2,hist) ggpairs(haberman[,1:3]) ggpairs(haberman, columns=1:3, ggplot2::aes(colour=V4, alpha=0.2)) Based on your findings in (a), comment on the appropriateness to apply \\(k\\)-NN, LDA and QDA. It is appropriateinappropriate to apply \\(k\\)-NN on this data. It is appropriateinappropriate to apply LDA on this data. It is appropriateinappropriate to apply QDA on this data. Use 10-fold cross-validation to select the optimal value of \\(k\\) in \\(k\\)-NN. Hint Check page 9 of week 3 lecture note for performing cross-validation. Remember that you should still split the data into training and test sets before using cross-validation. Suppose we use \\(3\\)-NN as the classifier and obtain the following prediction results. Calculate sensitivity, specificity, positive prediction rate, negative prediction rate and accuracy, assuming class 1 (i.e. the survival class) is the positive class and class 2 is the negative class. Comment on your results. test.pred &lt;- knn(train.X, test.X, train.Y, k=3) table(test.Y, test.pred) ## test.pred ## test.Y 1 2 ## 1 40 4 ## 2 14 4 Round all answers to two decimal places. Sensitivity = Specificity = Positive predictive rate = Negative predictive rate = Accuracy = "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
